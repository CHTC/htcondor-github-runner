#!/usr/bin/python3

import argparse
import http.client
import io
import json
import os
import posixpath
import pathlib
import shutil
import subprocess
import sys
import tarfile
import tempfile
import time

import classad
import htcondor
import htcondor.dags


class RunnerCreateException(Exception):
    pass

class RunnerQueryException(Exception):
    pass

class RunnerDeleteException(Exception):
    pass

class ExistingWorkingDir(Exception):
    pass


# The script that is launched from inside the VM
runTemplate = """#!/bin/sh

ORIG_DIR=/mnt/user
touch $ORIG_DIR/runner.stdout
touch $ORIG_DIR/runner.stderr
chown runner: $ORIG_DIR/runner.stdout
chown runner: $ORIG_DIR/runner.stderr
cd /home/runner/actions-runner
runuser -u runner -- ./config.sh --unattended --url {repo_url} --token {token} --name {runner_name} --replace --disableupdate --ephemeral >> $ORIG_DIR/runner.stdout 2>> $ORIG_DIR/runner.stderr
exec runuser -u runner -- ./run.sh >> $ORIG_DIR/runner.stdout 2>> $ORIG_DIR/runner.stderr
"""

# The service node template.  The dags library doesn't support SERVICE nodes
# so we need to hand craft the job
serviceTemplate = """
universe = local
executable = {exec}
+Arguments = strcat("service --group {group} --max-idle {max_idle} --org {org} --repo {repo} --token-file {token_file} --jobid ", DAGManJobId)
environment = PYTHONUNBUFFERED=1
log = {log_path}
output = {output_path}
error = {error_path}
on_exit_remove = false

queue 1
"""


def getRunnerToken(organization, repo, token):
    conn = http.client.HTTPSConnection("api.github.com")
    headers = {
        "Accept": "application/vnd.github+json",
        "Authorization": "Bearer " + token,
        "User-Agent": "htcondor-gh-runner",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    conn.request("POST", posixpath.join("/repos", organization, repo, "actions/runners/registration-token"), None, headers)
    
    response = conn.getresponse()
    data = response.read(1024*1024)
    conn.close()
    if response.status != 201:
        try:
            my_exception = RunnerCreateException(json.loads(data)['message'])
        except:
            raise RunnerCreateException(data.decode())
        raise my_exception
    return json.loads(data)['token']


def getRunnerStatus(prefix, organization, repo, token):
    # curl -L -s  -H "Accept: application/vnd.github+json"   -H "$(cat token_header_file)"   -H "X-GitHub-Api-Version: 2022-11-28"   https://api.github.com/repos/bbockelm/pelican/actions/runners
    conn = http.client.HTTPSConnection("api.github.com")
    headers = {
        "Accept": "application/vnd.github+json",
        "Authorization": "Bearer " + token,
        "User-Agent": "htcondor-gh-runner",
        "X-GitHub-Api-Version": "2022-11-28",
    }
    conn.request("GET", posixpath.join("/repos", organization, repo, "actions/runners"), None, headers)

    response = conn.getresponse()
    data = response.read(1024*1024)
    conn.close()
    if response.status != 200:
        try:
            my_exception = RunnerQueryException(json.loads(data)['message'])
        except:
            raise RunnerQueryException(data.decode())
        raise my_exception
    busy = set()
    idle = set()
    offline = set()
    for runner in json.loads(data)['runners']:
        if not runner["name"].startswith(prefix):
            continue
        if runner["status"] == "offline":
            offline.add(runner["id"])
        if runner["busy"]:
            busy.add(runner["id"])
        else:
            idle.add(runner["id"])
    return busy, idle, offline


def deleteRunner(organization, repo, runner_id, token):
    conn = http.client.HTTPSConnection("api.github.com")
    headers = {
        "Accept": "application/vnd.github+json",
        "Authorization": "Bearer " + token,
        "User-Agent": "htcondor-gh-runner",
        "X-GitHub-Api-Version": "2022-11-28",
    }

    print(f"Removing runner with ID {runner_id}")
    conn.request("DELETE", posixpath.join("/repos", organization, repo, "actions/runners", str(runner_id)), None, headers)
    response = conn.getresponse()
    data = response.read(1024*1024)
    conn.close()
    if response.status != 204:
        try:
            my_exception = RunnerDeleteException(json.loads(data)['message'])
        except:
            raise RunnerDeleteException(data.decode())
        raise my_exception


def deleteIdle(organization, repo, idle, max_idle, token):
    if len(idle) <= max_idle:
        return

    extra_runners = len(idle)-max_idle
    print(f"There are {extra_runners} idle runners that should be removed")

    for runner_id in list(idle)[:extra_runners]:
        deleteRunner(organization, repo, runner_id, token)


def deleteOffline(organization, repo, offline, token):
    for runner_id in offline:
        deleteRunner(organization, repo, runner_id, token)


def maintainIdleLoop(prefix, organization, repo, token, jobid, max_idle):
    while True:
        print("Will sleep for 60 seconds then check for idle runners")
        time.sleep(60)
        busy, idle, offline = getRunnerStatus(prefix, organization, repo, token)
        print(f"There were {len(idle)} idle runners and {len(offline)} offline runners")
        deleteOffline(organization, repo, offline, token)
        deleteIdle(organization, repo, idle, max_idle, token)
        schedd = htcondor.Schedd()
        desired_runners = int(len(busy) + max_idle)
        print(f"With {len(busy)} busy runners and target of {max_idle} idle, setting maximum runners to {desired_runners}")
        schedd.edit([f"{jobid}.0"], 'DAGMan_MaxJobs', str(desired_runners))


def generateDAG(prefix, working_dir, image_url, repo, org, token_file, max_runners, max_idle, cores, memory_mb):

    fname = image_url.split("/")[-1]
    runner_submit_description = htcondor.Submit({
        "executable":             f"$(JOB)", # Meaningless for a VM universe job; useful for status printing
        "universe":                "vm",
        "vm_type":                 "kvm",
        "vm_networking":            True,
        "vm_no_output_vm":          False,
        "vm_vnc":                   False,
        "vm_disk":                f"{fname}:vda:w:qcow2,input_disk_$(JOB).qcow2:vdb:w:qcow2",
        "request_disk":            "20GB",
        "request_cpus":             cores,
        "vm_memory":                memory_mb,
        "log":                    f"{prefix}-$(CLUSTER).log",
        "should_transfer_files":   "YES",
        "when_to_transfer_output": "ON_EXIT",
        "requirements":            "(TARGET.HasVirshDefaultNetwork == True)",
        "transfer_input_files":   f"{image_url},input_disk_$(JOB).qcow2",
        "transfer_output_files":   "input_disk_$(JOB).qcow2",
        "transfer_output_remaps":  '"input_disk_$(JOB).qcow2 = output_disk_$(JOB)_$(RETRY).qcow2"'
    })

    script_path = str(pathlib.Path(sys.argv[0]).absolute())
    working_dir_path = pathlib.Path(working_dir)
    prescript = htcondor.dags.Script(
        script_path,
        arguments = ["prescript", "-r", repo, "-o", org, "-t", str(pathlib.Path(token_file).absolute()),
                     "--jobid", "$JOB", "--retry", "$RETRY",
                     "--stdout", str((working_dir_path / "prescript.stdout").absolute()),
                     "--stderr", str((working_dir_path / "prescript.stderr").absolute()),
                     "--working-dir", working_dir_path.absolute()]
    )

    postscript = htcondor.dags.Script(
        script_path,
        arguments = ["postscript", "-r", repo, "-o", org, "-t", str(pathlib.Path(token_file).absolute()),
                     "--jobid", "$JOB", "--retry", "$RETRY",
                     "--stdout", str((working_dir_path / "postscript.stdout").absolute()),
                     "--stderr", str((working_dir_path / "postscript.stderr").absolute()),
                     "--working-dir", working_dir_path.absolute()]
    )

    dag = htcondor.dags.DAG()
    dag.layer(
       name = prefix,
       submit_description = runner_submit_description,
       vars = [{"node_name": f"Runner-{i}"} for i in range(max_runners)],
       retries = int(1e6),
       retry_unless_exit = 42,
       pre = prescript,
       post = postscript,
    )

    dag_dir = pathlib.Path(working_dir).absolute()
    try:
        dag_dir.mkdir()
    except FileExistsError:
        dir_str = str(dag_dir)
        raise ExistingWorkingDir(f"Working directory, {dir_str}, already exists; remove to reuse")

    dag_file = htcondor.dags.write_dag(dag, dag_dir, node_name_formatter=htcondor.dags.SimpleFormatter("_"))

    # The service node and supporting submit file need to be explicitly written due
    # to lack of support in the python library.
    service_sub = serviceTemplate.format(
        exec = script_path,
        max_idle = max_idle,
        group = prefix,
        org = org,
        repo = repo if repo else "''",
        token_file = pathlib.Path(token_file).absolute(),
        log_path = (dag_dir / f"service-$(Cluster).log").absolute(),
        error_path = (dag_dir / f"service-$(Cluster).stderr").absolute(),
        output_path = (dag_dir / f"service-$(Cluster).stdout").absolute(),
    )
    with open(dag_dir / "htcondor-GH-Runner-service.sub", "w") as fp:
        fp.write(service_sub)
    with open(dag_dir / "dagfile.dag", "a") as fp:
        fp.write("\nSERVICE htcondor-GH-Service htcondor-GH-Runner-service.sub\n")

    dag_submit = htcondor.Submit.from_dag(str(dag_file),{
        'batch-name': prefix,
        'maxjobs': max_idle # Start maximum number of runners at the maximum idle runners
    })

    os.chdir(dag_dir)
    schedd = htcondor.Schedd()
    submit_result = schedd.submit(dag_submit)
    print("GitHub runners were submitted as DAG with JobID %d.0" % submit_result.cluster())


def generateInputDisk(working_dir, repo_url, token, runner_name):
    contents = runTemplate.format(
        repo_url=repo_url,
        token=token,
        runner_name=runner_name,
    )

    output_file = (pathlib.Path(working_dir) / f"input_disk_{runner_name}.qcow2").absolute()

    with tempfile.NamedTemporaryFile("wb") as tarTempFile:
        with tarfile.open(None, "w:gz", tarTempFile) as tobj:
            tf = tarfile.TarInfo("run-job")
            tf.size = len(contents)
            tf.mode = 0O755
            tobj.addfile(tf, io.BytesIO(contents.encode()))
        tarTempFile.flush()

        with tempfile.NamedTemporaryFile("wb") as qcowTempFile:
            subprocess.run(["virt-make-fs", "-F", "qcow2", "--size", "100MB", tarTempFile.name, qcowTempFile.name], check=True)
            subprocess.run(["qemu-img", "convert", "-c", "-O", "qcow2", qcowTempFile.name, str(output_file)], check=True)


def countDags(prefix):
    schedd = htcondor.Schedd()
    return len(list(schedd.query(constraint='JobBatchName =?= %s' % classad.quote(prefix), projection=[])))


def prescriptMain(args):
    with open(args.token_file, 'r') as fp:
        token = fp.read(1024*1024)
        token = token.strip()

    registration_token = getRunnerToken(args.org, args.repo, token)

    repo_url = "https://github.com/" + posixpath.join(args.org, args.repo)
    generateInputDisk(args.working_dir, repo_url, registration_token, args.jobid)


def createFilename(fname, jobid, retry):
    fname_split = fname.rsplit(".", 1)
    if len(fname_split) == 2:
        return f"{fname_split[0]}-{jobid.replace(':', '_')}-{retry}.{fname_split[1]}"
    return f"{fname}-{jobid}-{retry}"


def postscriptMain(args):
     working_dir_path = pathlib.Path(args.working_dir).absolute()
     name_suffix = f"{args.jobid.lower().replace('-', '_').replace(':', '_')}_{args.retry}"
     output_disk_path = working_dir_path / f"output_disk_{args.jobid}_{args.retry}.qcow2"
     with tempfile.TemporaryDirectory() as td:
         subprocess.run(["guestfish", "add", str(output_disk_path.absolute()), ":", "run", ":", "mount", "/dev/sda", "/", ":", "copy-out", "/runner.stdout", "/runner.stderr", td])
         dirpath = pathlib.Path(td)
         shutil.move(dirpath / "runner.stdout", working_dir_path / f"runner_{name_suffix}.stdout")
         shutil.move(dirpath / "runner.stderr", working_dir_path / f"runner_{name_suffix}.stderr")
     os.unlink(str(output_disk_path))
     return 1


def helperMain():
    parser = argparse.ArgumentParser(description="Run a script for the HTCondor GitHub runners application")
    parser.add_argument("command", help="Helper command to run", choices=["prescript", "postscript", "service"])
    parser.add_argument('-r', '--repo', help="GitHub repo the runner will attach to", default="")
    parser.add_argument('-o', '--org', help="GitHub organization the runner will attach to", required=True)
    parser.add_argument('-t', '--token-file', help="Path to a file containing the GitHub PAT with admin permissions", required=True)
    parser.add_argument("-w", "--working-dir", help="Working directory for the DAG associated with the GitHub runners", default="working_dir")
    parser.add_argument("-j", "--jobid", help="Job ID for the GitHub runner name")
    parser.add_argument("--max-idle", help="Maximum number of idle runners to maintain", type=int)
    parser.add_argument("--retry", help="Retry number for the DAG node", type=int)
    parser.add_argument("--group", help="Unique group name for GitHub runners", default="htcondor-gh-runner")
    parser.add_argument("--stdout", help="Location of standard output for script")
    parser.add_argument("--stderr", help="Location of standard output for script")

    args = parser.parse_args()

    # DAG scripts notoriously have their stdout/err automatically set to
    # /dev/null; based on the flags, redirect these somewhere more useful.
    working_dir = pathlib.Path(args.working_dir)
    if args.stdout:
        stdout_name = createFilename(args.stdout, args.jobid, args.retry)
        new_stdout = working_dir / stdout_name
        with open(new_stdout, 'w') as fp:
            os.dup2(fp.fileno(), sys.stdout.fileno())
    if args.stderr:
        stderr_name = createFilename(args.stderr, args.jobid, args.retry)
        new_stderr = working_dir / stderr_name
        with open(new_stderr, 'w') as fp:
            os.dup2(fp.fileno(), sys.stderr.fileno())

    if args.command == "prescript":
        return prescriptMain(args)
    elif args.command == "postscript":
        return postscriptMain(args)
    elif args.command == "service":
        with open(args.token_file, 'r') as fp:
            token = fp.read(1024*1024)
            token = token.strip()
        return maintainIdleLoop(args.group, args.org, args.repo, token, args.jobid, args.max_idle)


def topMain():

    parser = argparse.ArgumentParser(description="Run self-hosted GitHub runners from within a HTCondor system")
    parser.add_argument('-r', '--repo', help="GitHub repo the runner will attach to", default="")
    parser.add_argument('-o', '--org', help="GitHub organization the runner will attach to", required=True)
    parser.add_argument('-t', '--token-file', help="Path to a file containing the GitHub PAT with admin permissions", required=True)
    parser.add_argument("-w", "--working-dir", help="Working directory for the DAG associated with the GitHub runners", default="working_dir")
    parser.add_argument("-i", "--image-url", help="URL to the GitHub runner VM image", default="osdf:///chtc/PUBLIC/bbockelm/alma_9.2-v4.qcow2")
    parser.add_argument("--max-runners", help="Maximum number of GitHub runners to execute", default=10, type=int)
    parser.add_argument("--max-idle", help="Maximum number of idle GitHub runners to maintain", default=3, type=int)
    parser.add_argument("--group", help="Unique group name for GitHub runners", default="htcondor-gh-runner")
    parser.add_argument("--runner-cores", help="Number of cores for each GitHub runner VM", default=1, type=int)
    parser.add_argument("--runner-memory", help="RAM (in megabytes) per GitHub runner VM", default=2048, type=int)

    args = parser.parse_args()

    if not os.path.exists(args.token_file):
        print(f"Token file, {args.token_file}, does not exist", file=sys.stderr)
        return 3

    if countDags(args.group):
        print(f"Cannot submit new runner group named {args.group}; one already exists in queue")
        return 2

    generateDAG(args.group, args.working_dir, args.image_url, args.repo, args.org, args.token_file, args.max_runners, args.max_idle,
        args.runner_cores, args.runner_memory)
    return 0


def main():
    # The same script serves as prescript, postscript, and the main driver.  We peek
    # at argv[1] to see what we should do in order to avoid dumping confusing help
    # options to the user
    if len(sys.argv) > 1 and sys.argv[1] in ["prescript", "postscript", "service"]:
        return helperMain()

    return topMain()

if __name__ == '__main__':
    sys.exit(main())
